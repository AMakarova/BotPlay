{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMakarova/BotPlay/blob/main/Bot_Play_FM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSCHHBL--hCB"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd 'drive/MyDrive/The Movie Dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFV3cFsTiuhq",
        "outputId": "bb2324d5-7d77-4392-b0dd-82631b759cb3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/The Movie Dataset'\n",
            "/content/drive/MyDrive/The Movie Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JxsZI_y97Zu"
      },
      "source": [
        "# Build Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgu50Xc2Y9ZF"
      },
      "source": [
        "## Process ratings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "import random\n",
        "import os.path\n",
        "import ast"
      ],
      "metadata": {
        "id": "z5kGzCJtnrJA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RATINGS_PATH = \"ratings.csv\"\n",
        "RECODED_RATINGS_PATH = 'ratings_recoded.csv'\n",
        "METADATA_PATH = \"movies_metadata.csv\"\n",
        "LINKS_PATH = \"links.csv\"\n",
        "\n",
        "TOP_MOVIE_COUNT = 100\n",
        "MIN_RATINGS = 10"
      ],
      "metadata": {
        "id": "mx7oxF9MohoN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(\n",
        "    path: str = RATINGS_PATH,\n",
        "    cols: Optional[list] = None,\n",
        "    ):\n",
        "  '''\n",
        "  Loads dataset as DataFrame, filters to specified list of columns\n",
        "  '''\n",
        "  print(f\"Loading {path} dataset...\")\n",
        "  dataset = pd.read_csv(path, low_memory=False)\n",
        "  if cols:\n",
        "    return dataset[cols]\n",
        "  else:\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Nj8hAsuAnrGK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_frequencies(ratings: pd.DataFrame):\n",
        "  '''\n",
        "  Prints user and movie count\n",
        "  '''\n",
        "  users = ratings['userId'].unique().size\n",
        "  movies = ratings['movieId'].unique().size\n",
        "  print(f\"Dataset contains {users} unique users and {movies} unique movies\")"
      ],
      "metadata": {
        "id": "SYeRiNPbo4zi"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_ratings(\n",
        "    ratings: pd.DataFrame, \n",
        "    top_movie_count: int = TOP_MOVIE_COUNT, \n",
        "    min_ratings: int = MIN_RATINGS\n",
        "    ) -> pd.DataFrame:\n",
        "  '''\n",
        "  Subsets ratings dataset\n",
        "  '''\n",
        "  print(f\"Subsetting ratings dataset...\")\n",
        "  movie_frequencies = ratings.groupby('movieId').count().sort_values('rating', ascending=False)\n",
        "  ratings = ratings[ratings['movieId'].isin(movie_frequencies.index[:top_movie_count])]\n",
        "                                            \n",
        "  user_frequencies = ratings[ratings['rating']>=4].groupby('userId').count()['rating']\n",
        "  ratings = ratings[ratings['userId'].isin(user_frequencies[user_frequencies>=min_ratings].index)]\n",
        "  print_frequencies(ratings)\n",
        "  return ratings"
      ],
      "metadata": {
        "id": "sA-tTxeFp7pl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binarise_dataset(ratings: pd.DataFrame) -> pd.DataFrame:\n",
        "  '''\n",
        "  Modify the dataset by:\n",
        "  * recoding movies with rating >=4 as positive class\n",
        "  * recoding movies with rating <4 as negative class\n",
        "  * sampling additional movies for the negative class from the set of unrated movies\n",
        "  '''\n",
        "  print(f\"Binarising ratings dataset and balancing classes.\")\n",
        "  recoded_ratings = pd.DataFrame()\n",
        "\n",
        "  for u in tqdm(ratings['userId'].unique()[:50000]): # free runtime seems to disconnect after this point\n",
        "    user_subset = ratings[ratings['userId']==u]\n",
        "    positive_class = user_subset[user_subset['rating']>=4]\n",
        "    positive_class.loc[:, 'rating'] = 1\n",
        "    negative_class = user_subset[user_subset['rating']<4]\n",
        "    negative_class.loc[:, 'rating'] = 0\n",
        "    if negative_class.shape[0] < positive_class.shape[0]:\n",
        "      len_diff = positive_class.shape[0] - negative_class.shape[0]\n",
        "      movie_pool = [m for m in ratings['movieId'].unique() if m not in user_subset['movieId']]\n",
        "      unrated_movie_sample = random.sample(movie_pool, len_diff)\n",
        "      unrated_movies = pd.DataFrame()\n",
        "      unrated_movies.loc[:, 'movieId'] = unrated_movie_sample\n",
        "      unrated_movies.loc[:, 'userId'] = u\n",
        "      unrated_movies.loc[:, 'rating'] = 0\n",
        "    recoded_ratings = pd.concat([recoded_ratings, positive_class, negative_class, unrated_movies])\n",
        "  print_frequencies(recoded_ratings)\n",
        "  return recoded_ratings"
      ],
      "metadata": {
        "id": "5crd_-IGzTU3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ratings(use_cache: bool = True) -> pd.DataFrame:\n",
        "  '''\n",
        "  Process the ratings dataset\n",
        "  '''\n",
        "  if use_cache and os.path.exists(RECODED_RATINGS_PATH):\n",
        "    ratings = load_dataset(RECODED_RATINGS_PATH, cols=['userId', 'movieId', 'rating'])\n",
        "  else:\n",
        "    ratings = load_dataset(path=RATINGS_PATH, cols=['userId', 'movieId', 'rating'])\n",
        "    print_frequencies(ratings)\n",
        "    ratings = subset_ratings(ratings)\n",
        "    ratings = binarise_dataset(ratings)\n",
        "    ratings.to_csv('ratings_recoded.csv')\n",
        "  return ratings"
      ],
      "metadata": {
        "id": "MleoucUFu_Ot"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_genres(metadata: pd.DataFrame) -> pd.DataFrame:\n",
        "  '''\n",
        "  Reformat genre data in columnar format\n",
        "  '''\n",
        "  genres = []\n",
        "  for line in metadata['genres']:\n",
        "    for genre in ast.literal_eval(line):\n",
        "      if genre['name'] not in genres:\n",
        "        genres.append(genre['name'])\n",
        "\n",
        "  genres = np.array(genres[:20]) # remove the erroneous genres\n",
        "  genre_matrix = np.zeros([metadata.shape[0], len(genres)], dtype=int)\n",
        "\n",
        "  for row_index, line in enumerate(tqdm(metadata['genres'])):\n",
        "    for genre in ast.literal_eval(line):\n",
        "      if genre['name'] in genres:\n",
        "        genre_index = np.where(genres==genre['name'])[0][0]\n",
        "        genre_matrix[row_index, genre_index] = 1\n",
        "\n",
        "  genre_matrix = pd.DataFrame(genre_matrix, columns=genres)\n",
        "  metadata = pd.concat([metadata, genre_matrix], axis=1)\n",
        "  metadata.drop('genres', axis=1, inplace=True)\n",
        "  return metadata"
      ],
      "metadata": {
        "id": "M82pOIcpxpdO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_in_metadata(ratings: pd.DataFrame) -> pd.DataFrame:\n",
        "  '''\n",
        "  Reformat genre data in columnar format\n",
        "  '''\n",
        "  metadata = load_dataset(path=METADATA_PATH, cols=['id', 'title', 'genres'])\n",
        "  metadata['id'] = pd.to_numeric(metadata['id'], errors='coerce')\n",
        "  metadata = parse_genres(metadata)\n",
        "  links = load_dataset(path=LINKS_PATH, cols=['movieId', 'tmdbId'])\n",
        "  links.rename(columns={'tmdbId':'id'}, inplace=True)\n",
        "  metadata = metadata.merge(links, how='inner', on='id')\n",
        "  ratings = ratings.merge(metadata.drop(columns=['id']), how='left', on='movieId').fillna(0)\n",
        "  return ratings"
      ],
      "metadata": {
        "id": "P_H6dJQn5a9R"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = process_ratings()\n",
        "ratings = merge_in_metadata(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKp8E5Ihu_Lh",
        "outputId": "5d724bc5-290a-4ea8-c3c7-346114c1215f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ratings_recoded.csv dataset...\n",
            "Loading movies_metadata.csv dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45466/45466 [00:01<00:00, 23883.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading links.csv dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation module"
      ],
      "metadata": {
        "id": "uW_NSudg6bE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / validation split"
      ],
      "metadata": {
        "id": "rbY4OEN36cSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "VAL_SIZE = 0.2\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "        ratings.drop(['rating', 'title'], axis=1),\n",
        "        ratings['rating'],\n",
        "        test_size=VAL_SIZE,\n",
        "        random_state=13,\n",
        ")"
      ],
      "metadata": {
        "id": "6x6yDhbU6BRq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training code"
      ],
      "metadata": {
        "id": "318LMo136l8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import logging\n",
        "from typing import Optional, Tuple, Union\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "SklearnInput = Union[list, np.ndarray, pd.DataFrame, pd.Series]\n",
        "\n",
        "\n",
        "class BinaryClassifierModel():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        batch_size: int,\n",
        "        learning_rate: float = 0.0001,\n",
        "        epochs: int = 1000,\n",
        "        optimiser: type = optim.Adam,\n",
        "        loss: nn.modules.loss._Loss = torch.nn.BCEWithLogitsLoss(),\n",
        "        silent = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialises the PyTorch based binary classifier object.\n",
        "\n",
        "        :param model: PyTorch model object\n",
        "        :param batch_size: batch size for model training\n",
        "        :param learning_rate: training learning rate\n",
        "        :param epochs: max number of training epochs\n",
        "        :param optimiser: optimiser object\n",
        "        :param loss: loss object\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.loss = loss.to(self.device)\n",
        "        self.optimiser = optimiser(self.model.parameters(), lr=learning_rate)\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.loss_history = {}\n",
        "        self.silent = silent\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X: SklearnInput,\n",
        "        y: SklearnInput,\n",
        "        eval_set: Optional[Tuple[SklearnInput, SklearnInput]] = None,\n",
        "        early_stopping_rounds: int = 10,\n",
        "        save_weights: bool = False,\n",
        "        epochs: Optional[int] = None,\n",
        "    ) -> \"BinaryClassifierModel\":\n",
        "        \"\"\"\n",
        "        Fits the binary classifier.\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: output dataset\n",
        "        :param eval_set: optional validation feature and output datasets\n",
        "        :param early_stopping_rounds: for how many epochs to train model after loss stops improving\n",
        "        :param save_weights: flag for saving weights to disk\n",
        "        :param epochs: max number of training epochs\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        train_iterator = self._dataset_iterator(X, y)\n",
        "        best_loss_epoch = None\n",
        "\n",
        "        if eval_set:\n",
        "            (X_val, y_val) = eval_set\n",
        "            val_iterator = self._dataset_iterator(X_val, y_val)\n",
        "            best_valid_loss = float(\"inf\")\n",
        "        else:\n",
        "            best_train_loss = float(\"inf\")\n",
        "\n",
        "        if epochs:\n",
        "            self.epochs = epochs\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            train_loss = self._train(train_iterator)\n",
        "            self.loss_history[epoch] = {}\n",
        "            self.loss_history[epoch]['train'] = train_loss\n",
        "\n",
        "            if eval_set:\n",
        "                valid_loss = self._evaluate(val_iterator)\n",
        "                self.loss_history[epoch]['val'] = valid_loss\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    best_valid_loss = valid_loss\n",
        "                    best_loss_epoch = epoch\n",
        "                    best_model = copy.copy(self.model)\n",
        "\n",
        "            else:\n",
        "                if train_loss < best_train_loss:\n",
        "                    best_train_loss = train_loss\n",
        "                    best_loss_epoch = epoch\n",
        "                    best_model = copy.copy(self.model)\n",
        "\n",
        "\n",
        "            if eval_set:\n",
        "                print(f\"\\tVal. loss: {valid_loss:.3f}\")\n",
        "\n",
        "            if save_weights:\n",
        "              torch.save(fm.state_dict(), 'fm_weights')\n",
        "              with open('fm_loss_history.pickle', 'wb') as handle:\n",
        "                  pickle.dump(self.loss_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            if epoch >= best_loss_epoch + early_stopping_rounds:\n",
        "                self.model = copy.copy(best_model)\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict(self, X: SklearnInput) -> pd.DataFrame: \n",
        "        \"\"\"\n",
        "        Returns predictions as a DataFrame.\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :return: a DataFrame with predicted ratings\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        return pd.DataFrame(\n",
        "            self.model(torch.Tensor(np.array(X))).sigmoid().detach().numpy(), # check if correct\n",
        "            columns=['predicted_rating'],\n",
        "        )\n",
        "\n",
        "    def param_count(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns number of trainable parameters in the model.\n",
        "\n",
        "        :return: number of trainable parameters\n",
        "        \"\"\"\n",
        "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "    def _dataset_iterator(\n",
        "        self,\n",
        "        X: SklearnInput,\n",
        "        y: SklearnInput,\n",
        "    ) -> DataLoader[object]:\n",
        "        \"\"\"\n",
        "        Converts scikit-learn style dataset into PyTorch DataLoader.\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: output dataset\n",
        "        :return: DataLoader iterator object\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.Tensor(np.array(X, dtype=\"float64\")),\n",
        "            # torch.Tensor(np.array(y, dtype=\"float64\").reshape((-1, 1))),\n",
        "            torch.Tensor(np.array(y, dtype=\"int\").reshape(-1, 1))\n",
        "        )\n",
        "\n",
        "        return DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def _train(self, iterator: DataLoader[object]) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Performs forward and backpropagation, updates model params\n",
        "        based on the loss.\n",
        "\n",
        "        :param iterator: dataset iterator\n",
        "        :return: average epoch loss\n",
        "        \"\"\"\n",
        "        epoch_loss = 0.0\n",
        "        self.model.train()\n",
        "\n",
        "        t = tqdm.tqdm(iterator, position=0, leave=True, disable=self.silent)\n",
        "\n",
        "        for (i, (x, y)) in enumerate(t):\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            self.optimiser.zero_grad()\n",
        "            y_pred = self.model(x)\n",
        "            loss = self.loss(y_pred, y)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            epoch_loss += loss.item()\n",
        "            t.set_description(f\"Train loss = {epoch_loss / (i+1):.3f}\")\n",
        "\n",
        "        return epoch_loss / len(iterator)\n",
        "\n",
        "    def _evaluate(self, iterator: DataLoader[object]) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Performs forward propagation only and calculates loss\n",
        "        without updating model params.\n",
        "\n",
        "        :param iterator: dataset iterator\n",
        "        :return: average epoch loss\n",
        "        \"\"\"\n",
        "        epoch_loss = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():  \n",
        "            for (x, y) in iterator:\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.loss(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "fyBw7qow6jY_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture"
      ],
      "metadata": {
        "id": "V_cFwXNN6rL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "20FXnTUy6o8v"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchFM(nn.Module):\n",
        "    def __init__(self, categorical_vars, X, k=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # upacking categorical vars\n",
        "        self.cat_cols = categorical_vars\n",
        "        self.dict_size = (X[categorical_vars].max()+1).to_list()\n",
        "        continuous_dim = X.shape[1] - len(self.cat_cols)\n",
        "        categorical_dim = sum(self.dict_size)\n",
        "        n = continuous_dim + categorical_dim\n",
        "       \n",
        "        # Initially we fill V with random values sampled from Gaussian distribution\n",
        "        # NB: use nn.Parameter to compute gradients\n",
        "        self.V = nn.Parameter(torch.randn(n, k),requires_grad=True)\n",
        "        self.lin = nn.Linear(n, 1)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # one-hot transformations\n",
        "        x_transformed = []\n",
        "\n",
        "        for i, col in enumerate(self.cat_cols):      \n",
        "            x_i = F.one_hot((x[:, i]).long(), self.dict_size[i])\n",
        "            if col == 'movieId':\n",
        "              x_i[:, 0] = 0\n",
        "            x_transformed.append(x_i)\n",
        "        if len(self.cat_cols) > 0:\n",
        "            x_cont = x[:, len(self.cat_cols):]\n",
        "        else:\n",
        "            x_cont = x\n",
        "        x_transformed.append(x_cont)\n",
        "        x_transformed = torch.cat(x_transformed, 1)\n",
        "\n",
        "        # FM calculations\n",
        "        out_1 = torch.matmul(x_transformed, self.V).pow(2).sum(1, keepdim=True) #S_1^2\n",
        "        self.x_transformed = x_transformed\n",
        "        out_2 = torch.matmul(x_transformed.pow(2), self.V.pow(2)).sum(1, keepdim=True) # S_2\n",
        "        out_inter = 0.5*(out_1 - out_2)\n",
        "        out_lin = self.lin(x_transformed)\n",
        "        out = out_inter + out_lin\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "px_liyu06v1s"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "categorical_vars = ['userId', 'movieId']"
      ],
      "metadata": {
        "id": "BBPADwTs60Aj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fm = TorchFM(categorical_vars, ratings.drop(['rating', 'title'], axis=1), k=20)\n",
        "\n",
        "model = BinaryClassifierModel(\n",
        "          model=fm,\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "D0AAL9Sv62GT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      eval_set=(X_val, y_val),\n",
        "      early_stopping_rounds=5,\n",
        "      save_weights=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn1g30qV65fU",
        "outputId": "315a6bbc-c489-4159-a5ce-ace51e20313f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss = 5.001:   1%|          | 1245/129969 [01:40<2:49:51, 12.63it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POREJCs37Esk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOCYk3loWksAtl9IWAEy8OX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}